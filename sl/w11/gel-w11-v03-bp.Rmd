---
title: "Model Selection"
author: "Peter von Rohr"
date: "29.04.2019"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::knit_hooks$set(hook_convert_odg = rmddochelper::hook_convert_odg)
```



## Why

* Many predictor variables are available
* Are all of them `relevant`?
* What is the meaning of `relevant` in this context?


## Example Dataset
```{r}
suppressPackageStartupMessages( require(dplyr) )
tbl_reg <- tibble::tibble(`Breast Circumference` = c(176, 177, 178, 179, 179, 180, 181, 182,183, 184),
                          `Body Weight` = c(471, 463, 481, 470, 496, 491, 518, 511, 510, 541))
n_nr_ani <- nrow(tbl_reg)
tbl_reg <- bind_cols(tibble::tibble(Animal = 1:n_nr_ani),
                     tbl_reg)
vec_randpred <- runif(n_nr_ani, 
                      min = min(tbl_reg$`Breast Circumference`), 
                      max = max(tbl_reg$`Breast Circumference`))
tbl_reg_aug <- bind_cols(tbl_reg, tibble(RandPred = round(vec_randpred, digits = 0)))
knitr::kable(tbl_reg_aug)
```


## No Relevance of Predictors

```{r}
plot(tbl_reg_aug$RandPred, tbl_reg_aug$`Body Weight`)
```


## Relevance of Predictors

```{r}
plot(tbl_reg_aug$`Breast Circumference`, tbl_reg_aug$`Body Weight`)
```


## Fitting a Regression Model 
\scriptsize
```{r}
lm_randpred <- lm(`Body Weight` ~ RandPred, data = tbl_reg_aug)
summary(lm_randpred)
```


## Fitting a Regression Model II
\scriptsize
```{r}
lm_bc <- lm(`Body Weight` ~ `Breast Circumference`, data = tbl_reg_aug)
summary(lm_bc)
```


## Multiple Regression
\scriptsize
```{r}
lm_full <- lm(`Body Weight` ~ `Breast Circumference` + RandPred, data = tbl_reg_aug)
summary(lm_full)
```


## Which model is better?
Why not taking all predictors?

* Additional parameters must be estimated from data
* Predictive power decreased with too many predictors (cannot be shown for this data set, because too few data points)
* Bias-variance trade-off


## Bias-variance trade-off
* Assume, we are looking for optimum prediction

$$s_i = \sum_{r=1}^q \hat{\beta}_{j_r}x_{ij_r}$$
with $q$ relevant predictor variables

* Average mean squared error of prediction $s_i$ 

$$MSE = n^{-1}\sum_{i=1}^n E\left[(m(x_i) - s_i)^2 \right]$$
where $m(.)$ denotes the linear function of the unknown true model. 

## Bias-variance trade-off II

* MSE can be split into two parts

$$MSE = n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2 
  + n^{-1}\sum_{i=1}^n var(s_i)
$$

where $n^{-1}\sum_{i=1}^n \left( E\left[s_i \right] - m(x_i) \right)^2$ is called the squared __bias__

* Increasing $q$ leads to reduced bias but increased variance ($var(s_i)$)
* Hence, find $s_i$ such that MSE is minimal
* Problem: cannot compute MSE because $m(.)$ is not known 

$\rightarrow$ estimate MSE


## Mallows $C_p$ statistic
* For a given model $\mathcal{M}$, $SSE(\mathcal{M})$ stands for the residual sum of squares.
* MSE can be estimated as

$$\widehat{MSE} = n^{-1} SSE(\mathcal{M}) - \hat{\sigma}^2 + 2 \hat{\sigma}^2 |\mathcal{M}|/n$$

where $\hat{\sigma}^2$ is the estimate of the error variance of the full model, $SSE(\mathcal{M})$ is the residual sum of squares of the model $\mathcal{M}$, $n$ is the number of observations and $|\mathcal{M}|$ stands for the number of predictors in $\mathcal{M}$

$$C_p(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat{\sigma}^2} - n + 2 |\mathcal{M}|$$


## Searching The Best Model
```{r}
n_nr_pred <- 16
```

* Exhaustive search over all sub-models might be too expensive
* For $p$ predictors there are $2^p - 1$ sub-models
* With $p=`r n_nr_pred`$, we get $`r 2^n_nr_pred - 1`$ sub-models

$\rightarrow$ step-wise approaches

## Forward Selection
1. Start with smallest sub-model $\mathcal{M}_0$ as current model
2. Include predictor that reduces SSE the most to current model
3. Repeat step 2 until all predictors are chosen

$\rightarrow$ results in sequence $\mathcal{M}_0 \subseteq \mathcal{M}_1 \subseteq \mathcal{M}_2 \subseteq \ldots$ of sub-models

4. Out of sequence of sub-models choose the one with minimal $C_p$


## Backward Selection
1. Start with full model $\mathcal{M}_0$ as the current model
2. Exclude predictor variable that increases SSE the least from current model
3. Repeat step 2 until all predictors are excluded (except for intercept)

$\rightarrow$ results in sequence $\mathcal{M}_0 \supseteq \mathcal{M}_1 \supseteq \mathcal{M}_2 \supseteq \ldots$ of sub-models

4. Out of sequence choose the one with minimal $C_p$


## Considerations
* Whenever possible, choose __backward__ selection, because it leads to better results
* If $p \geq n$, only forward is possible, but then consider LASSO


## Alternative Selection Criteria
* AIC or BIC, requires distributional assumptions. 
* AIC is implemented in `MASS::stepAIC()` 
* Adjusted $R^2$ is a measure of goodness of fit, but sometimes is not conclusive when comparing two models
* Try in exercise
