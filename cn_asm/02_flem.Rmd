# Fixed Linear Effects Models {#asm-flem}
```{r mrmt-flem-reset, include=FALSE}
mrmt$set_this_rmd_file(ps_this_rmd_file = ifelse(rstudioapi::isAvailable(), 
                                                 rstudioapi::getSourceEditorContext()$path, 
                                                 rprojroot::thisfile()))
```

## Other Resources {#asm-flem-other-resources}
This chapter is based on the work of `r mrmt$add("Buehlmann2014")`. Apart from that there are many other resources for the topic of `Multiple Linear Regressions`. An interesting online book is `r mrmt$add("Lilja2016")`. 


## Motivation {#asm-flem-motivation}
Why is the topic of `fixed linear effects models` (FLEM) important for the analysis of genomic data? This question is best answered when looking at the data. In chapter \@ref(asm-intro), we saw that genomic breeding values can either be estimated using a two-step procedure (see section \@ref(asm-two-step-approach)) or by a single step approach (see section \@ref(asm-single-step-approach)). At the moment, we assume that we are in the first step of the two step approach where we estimate the $a$ effects in a reference population or alternatively we have a perfect data set with all animals genotyped and with a phenotypic observation in a single step setting. Both situations are equivalent when it comes to the structure of the underlying dataset and with respect to the proposed model to analyse the data.


## Data {#asm-flem-data}
As already mentioned in section \@ref(asm-flem-motivation), we are assuming that we have a perfect data set for a given population of animals. That means each animal $i$ has a phenotypic observation $y_i$ for a given trait of interest. Furthermore, we assume to just have a map of three SNP markers. The marker loci are called $G$, $H$ and $I$. Each of the markers has just two alleles. Figure \@ref(fig:datastucturegbv) tries to illustrate the structure of a dataset used to estimate GBV.

```{r datastucturegbv, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Structure of Dataset To Estimate GBV"}
#rmddochelper::use_odg_graphic(ps_path = "odg/datastucturegbv.odg")
knitr::include_graphics(path = "odg/datastucturegbv.png")
```
 
As can be seen from Figure \@ref(fig:datastucturegbv) each of the $N$ animals have known genotypes for all three SNP markers and they all have a phenotypic observation $y_i \quad (i = 1, \cdot, N)$. Because we are assuming each SNP marker to be bi-allelic, there are only three possible marker genotypes at every marker position. Hence marker genotypes are discrete entities with a fixed number of levels. Due to the nature of the SNP marker genotype data, we can already say that they could be modeled as fixed effects in a fixed linear effects model. More details about the model will follow in section \@ref(asm-flem-model).


## Model {#asm-flem-model}
The goal of our data analysis using the dataset described in section \@ref(asm-flem-data) is to come up with estimates for genomic breeding values for all animals in our dataset. The genomic breeding values will later be used to rank the animals. The ranking of the animals according to the GBV is used to select the parents of the future generation of livestock animals. It probably makes sense to distinguish between two different types of models that we have to set up. On the one side we need a model that describes the underlying genetic architecture which is present in our dataset. We will be using a so-called __genetic__ model to describe this. On the other side, we have at some point being able to get estimates for the GBVs which requires a __statistical__ model which is able to estimate unknown parameters as a function of observed data. In the end, we will realize that the two models are actually the same model but they are just different ways of looking at the same structure of underlying phenomena.


### Genetic Model {#asm-flem-genetic-model}
The availability of genomic information for all animals in the dataset makes it possible to use a polygenic model. In contrast to an infinitesimal model, a polygenic model uses a finite number of discrete loci to model the genetic part of an expressed phenotypic observation. From quantitative genetics (see e.g. [@Falconer1996] for a reference) we know that every phenotypic observation $y$ can be separated into a genetic part $g$ and an environmental part $e$. This leads to the very simple genetic model 

\begin{equation}
  y = g + e
  (\#eq:simplegeneticmodel)
\end{equation}

The environmental part can be split into some fixed known systematic factors such as `herd`, `season effects`, `age` and more and into a random unknown part. The systematic factors are typically grouped into a vector of fixed effects called $\beta$. The unknown environmental random part is usually called $\epsilon$. This allows to re-write the simple genetic model in \@ref(eq:simplegeneticmodel) as

\begin{equation}
  y = \beta + g + \epsilon
  (\#eq:envdecompgeneticmodel)
\end{equation}

The genetic component $g$ can be decomposed into contributions from the finite number of loci that are influencing the observation $y$. In our example dataset (see Figure \@ref(fig:datastucturegbv)) there are three loci^[Implicitly, we are treating the SNP-markers to be identical with the underlying QTL. But based on the fact that we have very many SNPs spread over the complete genome, there will always be SNP sufficiently close to every QTL that influences a certain trait. But in reality the unknown QTL affect the traits and not the SNPs.] that are assumed to have an effect on $y$. Ignoring any interaction effects between the three loci, we can decompose the overall genetic effect $g$ into the some of the genotypic values of each locus. Hence 

\begin{equation}
  g = \sum_{j=1}^k g_j
  (\#eq:decompgeneticeffect)
\end{equation}

where for our example $k$ is equal to three^[In reality $k$ can be $1.5*10^5$ for some commercial SNP chip platforms. When working with complete genomic sequences, $k$ can also be in the order of $3*10^7$.]. 

Considering all SNP loci to be purely additive which means that we are ignoring any dominance effects, the genotypic values $g_j$ at any locus $j$ can just take one of the three values $-a_j$, $0$ or $+a_j$ where $a_j$ corresponds to the $a$ value from the mono-genic model (see Figure \@ref(fig:monogenicsnpmodel)). For our example dataset the genotypic value for each SNP genotype is given in the following table.

```{r genotypicvalue, echo=FALSE, results='asis'}
tbl_genoval <- tibble::tibble(`SNP Locus` = c(rep("$SNP_1$", 3),
                                              rep("$SNP_2$", 3),
                                              rep("$SNP_3$", 3)),
                              Genotype = c("$G_1G_1$","$G_1G_2$","$G_2G_2$",
                                           "$H_1H_1$","$H_1H_2$","$H_2H_2$",
                                           "$I_1I_1$","$I_1I_2$","$I_2I_2$"),
                              `Genotypic Value` = c("$a_1$", "$0$", "$-a_1$",
                                                    "$a_2$", "$0$", "$-a_2$",
                                                    "$a_3$", "$0$", "$-a_3$"))
knitr::kable(tbl_genoval,
             booktabs = TRUE,
             longtable = TRUE,
             caption = "Genotypic Values For All Three SNP-Loci",
             escape = FALSE)
```

From the Table \@ref(tab:genotypicvalue) we can see that always the allele with subscript $1$ is taken to be that with the positive effect. Combining the information from Table \@ref(tab:genotypicvalue) together with the decomposition of the genotypic value $g$ in \@ref(eq:decompgeneticeffect), we get

\begin{equation}
  g = M \cdot a
  (\#eq:genotypicvalueintermsofa)
\end{equation}

where $M$ is an indicator matrix taking values of $-1$, $0$ and $1$ depending on the SNP marker genotype and $a$ is a vector of $a$ values. Combining the decomposition in \@ref(eq:genotypicvalueintermsofa) together with the basic genetic model in \@ref(eq:envdecompgeneticmodel), we get

\begin{equation}
  y = \beta + M \cdot a + \epsilon
  (\#eq:finalgeneticmodel)
\end{equation}

The result obtained in \@ref(eq:envdecompgeneticmodel) is the fundamental decomposition of the phenotypic observation $y$ into a genetic part represented by the SNP marker information ($M$) and an environmental part ($\beta$ and $\epsilon$). The $a$ values are unknown an must be estimated. The estimates of the $a$ values will then be used to predict the GBVs. How this estimation procedure works is described in the next section \@ref(asm-flem-statistical-model). 


### Statistical Model {#asm-flem-statistical-model}
When looking at the fundamental decomposition given in the statistical model presented in \@ref(eq:finalgeneticmodel) from a statistics point of view, the model in \@ref(eq:finalgeneticmodel) can be interpreted as __fixed linear effects model__ (FLEM). FLEM represent a class of linear models where each model term except for the random residual term is a fixed effect. 

Using the decomposition given in our genetic model (see equation \@ref(eq:finalgeneticmodel)) for our example dataset illustrated in Figure \@ref(fig:datastucturegbv), every observation $y_i$ of animal $i$ can be written as

\begin{equation}
  y_i = W_i \cdot \beta + M_i \cdot a + \epsilon_i
  (\#eq:basisstatisticalmodel)
\end{equation}

where 

* $y_i$ is the observation of animal $i$
* $\beta$ is a vector of unknown systematic environmental effects
* $W_i$ is an indicator row vector linking $\beta$ to $y_i$
* $a$ is a vector of unknown additive allele substitution effects ($a$ values)
* $M_i$ is an indicator row vector encoding the SNP genotypes of animal $i$ and
* $\epsilon_i$ is the random unknown environmental term belonging to animal $i$

In the following section, we write down the definition of a FLEM and compare it to the statistical model given in \@ref(eq:basisstatisticalmodel). 


## Definition of FLEM {#asm-flem-definition}
The multiple fixed linear effects model is defined as follows.

```{definition, label="defflem", name="Fixed Linear Effects Model"}
In a fixed linear effects model, every observation $i$ in a dataset is characterized by a __response variable__ and a set of __predictors__. Up to some random errors the response variable can be expressed as a linear function of the predictors. The proposed linear function contains unknown parameters. The goal is to estimate both the unknown parameters and the error variance.
```


### Terminology {#asm-flem-terminology}
For datasets where both the predictors and the response variables are on a continuous scale, which means that they correspond to measured quantities such as body weight, breast circumference or milk yield, the model is referred to as __multiple linear regression model__. Because the statistical model in \@ref(eq:basisstatisticalmodel) contains the SNP genotypes as discrete fixed effects, we are not dealing with a regression model but with a more general fixed linear effects model. 


### Model Specification {#asm-flem-model-specification}
An analysis of the model given in \@ref(eq:basisstatisticalmodel) shows that it exactly corresponds to the definition \@ref(def:defflem). In this equivalence, the observation $y_i$ corresponds to the response variable. Furthermore, the unknown environmental term $\epsilon$ corresponds to the random residual part in the FLEM. Except for the random residuals the response variable $y_i$ is a linear function of the fixed effects which corresponds to all systematic environmental effects and to all SNP genotype effects. 

For the description of how to estimate the unknown parameter $\beta$ and $a$ in the model \@ref(eq:basisstatisticalmodel), it is useful to combine $\beta$ and $a$ into a single vector of unknown parameters and we call it $b$.

\begin{equation}
  b = \left[ \begin{array}{c} \beta \\ a \end{array} \right]
  (\#eq:combinefixedeffects)
\end{equation}

Taking the equations as shown in \@ref(eq:basisstatisticalmodel) for all observations ($i=1, \ldots, N$) and expressing them in matrix-vector notation, we get

\begin{equation}
 y = Xb + \epsilon
 (\#eq:flemmatrixvector)
\end{equation}

where 

* $y$ is the vector of $N$ observations
* $b$ is the vector of all unknown fixed effects
* $X$ is the incidence matrix linking the parameters of $b$ to $y$
* $\epsilon$ is the vector of random residuals

The incidence matrix $X$ in \@ref(eq:flemmatrixvector) can be composed from the matrices $W$ and $M$ by concatenating the latter two matrices, i.e., 

\begin{equation}
  X = \left[ \begin{array}{cc} W  &  M  \end{array} \right]
  (\#eq:composematrixx)
\end{equation}


## Parameter Estimation Using Least Squares {#asm-flem-parameter-estimation}
The method of parameter estimation is explained using the simpler case of a regression model. That means both the predictors and the response variables are on a continuous scale. As a further simplification, we assume that there is only one predictor variable and one response variable. The predictor variable is called $x$ and the response variable is called $y$. The model is still the same as shown in \@ref(eq:flemmatrixvector). The matrix $X$ has just one column with the measured values of the predictor variable and $b$ is just a scalar unknown parameter. The vector $y$ contains the observed values for the response values. 

The goal of the analysis of the simple dataset is to find an estimate of the scalar $b$ such that the linear combination of $X$ and $b$ best explains the values in $y$. How we can find such an estimation procedure that allows us to calculate an estimate of $b$ is explained using a small example data set in the following subsection.


### An Example Dataset {#asm-flem-regression-example}
A widely use example dataset for such a simple regression analysis in animal breeding consists of measurements of `body weight` (BW) and `breast circumference` (BC) for a given group of animals.

```{r dataregression, echo=FALSE, results='asis'}
tbl_reg <- tibble::tibble(Animal = c(1:10),
                          `Breast Circumference` = c(176, 177, 178, 179, 179, 180, 181, 182,183, 184),
                          `Body Weight` = c(471, 463, 481, 470, 496, 491, 518, 511, 510, 541))
knitr::kable(tbl_reg,
             booktabs = TRUE,
             longtable = TRUE,
             caption = "Dataset for Regression of Body Weight on Breast Circumference for ten Animals")
```

The dataset shown above is taken from Table 9.1 in `r mrmt$add("Essl1987")`. One of the possible reasons for fitting a regression from BW on BC is that the latter is easier to measure. The measured values of BC can be used to predict BW once we have determined the regression coefficient. For this prediction, we use BW as response variable $y$ and BC as predictor variable $x$. This leads to the regression model

\begin{equation}
  y = x * b + \epsilon
  (\#eq:regressionbwonbc)
\end{equation}

where $y$ is the vector of body weights and $x$ is the vector of breast circumferences. $b$ is a scalar value which is unknown and $\epsilon$ is the vector of random unknown error terms. The goal is to determine $b$ such that the predictor variable best explains the response variable. How $b$ is determined is explained with the following plot.

```{r generateregressionbwonbc, echo=FALSE, eval=FALSE}
require(ggplot2)
ggplot(data = tbl_reg, aes(x = `Breast Circumference`, y = `Body Weight`)) + 
  geom_point(color = 'blue') + 
  geom_smooth(method = "lm", se = FALSE, color = 'red')
```

```{r showregressionbwonbc, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Regression of Body Weight On Breast Circumference"}
#rmddochelper::use_odg_graphic(ps_path = "odg/showregressionbwonbc.odg")
knitr::include_graphics(path = "odg/showregressionbwonbc.png")
```

In Figure \@ref(fig:showregressionbwonbc) the blue points correspond to the data points given by the dataset shown in Table \@ref(tab:dataregression). The red line corresponds to the regression line defined by the unknown regression parameter $b$. The distance between the data points to the projection in the direction of the $y$-axis corresponds to the residual $r$. For a given data point $i$, the residual $r_i$ is computed as

\begin{equation}
  r_i = y_i - x_i * \hat{b}
  (\#eq:definitionresidual)
\end{equation}

where $\hat{b}$ denotes a concrete estimated value of $b$. For a different choice of a value of $\hat{b}$, different values for the residuals $r_i$ can be computed. Our goal is to find the value of $\hat{b}$ that results in the smallest residuals $r_i$. In order to avoid cancellation of positive and negative values of the residuals, the $r_i$ values are squared and added. This sum of the squared residuals is used as a measure of how good a given regression line determined by $\hat{b}$ fits a given set of data points. Because we want to have a good fit this means that the sum of the squared residuals should be as small as possible.

The method that determines $\hat{b}$ such that the sum of the squared residuals is minimal is called __Least Squares__. In a general formula with more than one predictor variables we can write the least squares estimate $\hat{b}_{LS}$ as 

\begin{equation}
  \hat{b}_{LS} = argmin_b ||y - Xb||^2
  (\#eq:leastsquaresestimate)
\end{equation}

where $||.||$ denotes the Euclidean norm. The estimate $\hat{b}_{LS}$ can be found by finding the minimum of $||y - Xb||^2$. The minimum of $||y - Xb||^2$ is found by first taking the derivative with respect to $b$ and the setting that derivative to $0$. The derivative of $||y - Xb||^2$ with respect to $b$ can be computed as follows

\begin{equation}
  LS = ||y - Xb||^2 = (y - Xb)^T(y - Xb) = y^Ty - y^TXb - b^TX^Ty + b^TX^TXb
  (\#eq:computels)
\end{equation}

The derivative of $LS$ with respect to $b$ is

\begin{equation}
  \frac{\partial LS}{\partial b} = -y^TX - y^TX + 2*b^TX^TX
  (\#eq:partiallswrtb)
\end{equation}

The minimum is found by setting $\frac{\partial LS}{\partial b}$ to $0$. 

\begin{equation}
  \frac{\partial LS}{\partial b} = -y^TX - y^TX + 2*\hat{b}^TX^TX = 0
  (\#eq:minimumls)
\end{equation}

From equation \@ref(eq:minimumls), we get the so-called least squares __Normal Equations__ for $\hat{b}$.

\begin{equation}
   X^TX\hat{b} = X^Ty
  (\#eq:lsnormalequations)
\end{equation}

For a regression model, we know that $X$ has full column rank^[In a regression model, all values in the matrix $X$ are real values. Hence no column of $X$ will be a linear combination of any other columns and therefore $X$ has full column rank.]. That means we can solve the normal equations \@ref(eq:lsnormalequations) explicitly for $\hat{b}$.

\begin{equation}
   \hat{b} = (X^TX)^{-1}X^Ty
  (\#eq:solutionhatb)
\end{equation}

Equation \@ref(eq:solutionhatb) presents a solution to the estimation problem of the unknown parameter $b$ in the regression problem. There is one additional unknown parameter that we have not mentioned so far. The regression model contains the random error terms $\epsilon$. Because $\epsilon$ is random, we have to specify the expected value and the variance. The error terms are deviations of the predicted values from the observed data points. Hence the expected values $E\left[\epsilon \right]$ must be $0$. The variance $\sigma^2$ of the error terms is an additional unknown parameter that has to be estimated from the data. One way of estimating the error variance from the data is shown in subsection \@ref(asm-flem-error-variance).


### Variance of Errors {#asm-flem-error-variance} 
The least squares procedure itself does not yield an estimate of the error variance $\sigma^2$. But the estimate of $\sigma^2$ based on the residuals is often declared to be the `least squares estimate` of $\sigma^2$. The residuals $r_i$ as defined in \@ref(eq:definitionresidual) are estimates of the error terms $\epsilon_i$. As a matter of fact the residuals can be used to estimate $\sigma^2$. This estimate is given by

\begin{equation}
  \widehat{\sigma^2} = \frac{1}{n-p} \sum_{i=1}^n r_i^2
  (\#eq:lsestimateerrorvariance)
\end{equation}

The factor $(n-p)^{-1}$ in \@ref(eq:lsestimateerrorvariance) is used, because it leads the estimate $\widehat{\sigma^2}$ to be unbiased, which means $E\left[\widehat{\sigma^2} \right] = \sigma^2$.


## Different Types of Linear Regressions {#asm-flem-types-of-regression}
### Regression Through The Origin {#asm-flem-regression-origin}
The regression model as it was proposed in \@ref(eq:regressionbwonbc) for the dataset of body weight and breast circumference defines a line in the $x-y$-plane. This line shown in Figure \@ref(fig:showregressionbwonbc). What is not shown in the plot, but what becomes clear from the model is that the regression line goes through the origin of the coordinate system. Mathematically the origin is given by $x=0$ and $y=0$. In this regression model, the origin is the fixed point which is on the regression line. The fixed point together with the estimated regression coefficient $\hat{b}$ uniquely define the regression line. From a geometrical point of view the estimated regression coefficient defines the slope of the regression line.


### Regression With Intercept {#asm-flem-regression-intercept}
Depending on the data analysed with a regression model, it does not make sense to force the regression line to run through the origin. This can be avoided by including an additional fixed term in the regression model. This term is called the __intercept__. A regression model with an intercept can be written as

\begin{equation}
  y_i = b_0 + x_i * b_1 + \epsilon_i
  (\#eq:regressionintercept)
\end{equation}

The term $b_0$ corresponds to the value of the response variable $y$ when the value of the predictor $x$ is $0$. Then the fixed point of the regression line is no longer the origin, but the point $x = 0$ and $y = \widehat{b_0}$. The slope of the regression line is determined by $\widehat{b_1}$. In matrix-vector notation the intercept $b_0$ is added to the vector of unknown parameters $b$ and the design-matrix $X$ has to be augmented by a column of all ones on the left.


### Regression With Transformed Predictor Variables {#asm-flem-regression-transformed-predictors}
Regression models can also contain different transformations of the predictor variables. As an example, we can include any higher order polynomial functions of predictor variables such as 

\begin{equation}
  y_i = b_0 + b_1 * x_i + b_2 * x_i^2 + \cdots + b_k * x_i^k + \epsilon_i
  (\#eq:polynomialregression)
\end{equation}

Although the model \@ref(eq:polynomialregression) contains non-linear functions of the predictors $x_i$, the function is still linear in the unknown parameters $b_j$ ($j = 1, \ldots k$) and hence the model \@ref(eq:polynomialregression) is still a linear regression model.

Transformations of the predictor variables are not restricted to polynomial functions. Many different kinds of transformations are possible. An example is shown in the following equation

\begin{equation}
  y_i = b_0 + b_1 * log(x_i) + b_2 * sin(\pi x_i) + \epsilon_i
  (\#eq:generalregression)
\end{equation}


## Predictions {#asm-flem-prediction}
One goal of estimating the regression coefficient was that we want to be able to predict the response based on concrete values of the predictor variables. For our example with the body weight and the breast circumference, this means that we want to measure the breast circumference of an animal for which we do not know the body weight. Then based on the estimated regression coefficient, we want to be able to predict the body weight of that animal.

The computation of the regression coefficient for the dataset shown in Table \@ref(tab:dataregression) will be the topic of an exercise. But let us assume that we have computed the value of $\hat{b}$, then the predicted value of the body weight $\widehat{y_s}$ for an animal $s$ is computed based on the measured breast circumference $x_s$ of animal $s$ as follows

\begin{equation}
  \widehat{y_s} = \hat{b} * x_s
  (\#eq:predictbwonbc)
\end{equation}

It has to be noted that the prediction $\widehat{y_s}$ is only valid, if the measured value $x_s$ is close to the measured predictors that were used to estimate $\hat{b}$. For our example with body weight and breast circumference, we could not use the same regression line to predict the body weight for calves, if $\hat{b}$ was estimated with data of adult bulls.




